---
title: "IMT 573: Problem Set 4 - Scrape data and get it in shape"
author: "Feroz Khan"
date: 'Due: Tuesday, October 29, 2024 by 10:00PM PT'
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
---

<!-- This syntax can be used to add comments that are ignored during knitting process. -->

##### Collaborators: <!-- BE SURE TO LIST ALL COLLABORATORS HERE! -->

##### Instructions:

Before beginning this assignment, please ensure you have access to R and RStudio. 

1. Download the `pedi.Rmd` file from Canvas. Open `problem_set4.Rmd` in RStudio and supply your solutions to the assignment by editing `problem_set4.Rmd`. 

2. Replace the "Insert Your Name Here" text in the `author:` field with your own full name. Any collaborators must be listed on the top of your assignment. Collaboration shouldn't be confused with group project work (where each person does a part of the project). Working on problem sets should be your individual contribution.

3. Be sure to include well-documented (e.g. commented) code chucks, figures, and clearly written text chunk explanations as necessary. Any figures should be clearly labeled and appropriately referenced within the text. Be sure that each visualization adds value to your written explanation; avoid redundancy -- you do not need four different visualizations of the same pattern.

4. All materials and resources that you use (with the exception of lecture slides) must be appropriately referenced within your assignment. In particular, note that Stack Overflow is licenses as Creative Commons (CC-BY-SA). This means you have to attribute any code you refer from SO.

5. Partial credit will be awarded for each question for which a serious attempt at finding an answer has been shown. But please **DO NOT** submit pages and pages of hard-to-read code and attempts that is impossible to grade. That is, avoid redundancy. Remember that one of the key goals of a data scientist is to produce coherent reports that others can easily follow.  Students are \emph{strongly} encouraged to attempt each question and to document their reasoning process even if they cannot find the correct answer. If you would like to include R code to show this process, but it does not run without errors you can do so with the `eval=FALSE` option as follows:

```{r example chunk with a bug, eval=FALSE}
a + b # these object dont' exist 
# if you run this on its own it with give an error
```

6. When you have completed the assignment and have **checked** that your code both runs in the Console and knits correctly when you click `Knit PDF`, rename the knitted PDF file to `ps1_ourLastName_YourFirstName.pdf`, and submit the PDF file on Canvas.

7.  Collaboration is often fun and useful, but each student must turn in an individual write-up in their own words as well as code/work that is their own.  Regardless of whether you work with others, what you turn in must be your own work; this includes code and interpretation of results. The names of all collaborators must be listed on each assignment. Do not copy-and-paste from other students' responses or code.

##### Setup

In this problem set you will need, at minimum, the following R packages. Install missing packages, if required.

```{r setup, include=FALSE}
# load required libraries
library(dplyr)
library(tidyverse)
library(rvest)
library(xml2)
library(ggplot2)
library(viridis)
```

# Instructions

# 1. Scrape the web

This question asks you to extract for tables of data from the internet. In particular, you are going to fetch the location of all tall mountains from wikipedia, and plot these on the world map!

Briefly, your tasks are the following:

- Load the wikipedia list of mountains by elevation
- Find all mountains taller than 6800 meters
- Follow the link to the wikipedia page of that mountain, and extract the geographic coordinates (longitude/latitude).
- Plot these locations on the world map.

There is a brief introduction to rvest and webscraping in R at http://faculty.washington.edu/otoomet/machinelearning-R/web-scraping.html.
Below is the more detailed task lists. The tasks are listed in order that you may find useful to follow when developing your code, but you may also combine different tasks in a different order.



# 1.1 Parse the list of mountains (30pt)

This is a suggested approach for you to follow. You may do the tasks slightly differently.

1. (4pt) Load the wikipedia list of mountains by height (https://en.wikipedia.org/wiki/List_of_mountains_by_elevation) and parse it with rvest library.
2. (4pt) Find all the tables there in the html.
3. (7pt) Find the table headers, and determine which columns are mountain names, heights, and where are the links to the individual mountain pages.
Note: if you analyze the page in your browser, you’ll probably get javascript-aware version. Down- loading in R gives you the non-javascript version, the table headers differ somewhat between these two versions.
4. (13pt) Create a data frame that contains names and heights of the mountains above 6800m, and the links to the corresponding wikipedia pages. You’ll add longitude and latitude for each mountain in this data frame later.
Hint: you should have 100+ mountains with valid links.
5. (2pt) Print a small sample of your data frame to see that it looks reasonable.

##### Solution 1.1

1. \textbf{\textcolor{red}{Solution:}}

###### Insert Response

```{r }
link = "https://en.wikipedia.org/wiki/List_of_mountains_by_elevation"

#Reading the HTML page and pulling the first table
web_page = read_html(link)
mountains_table = html_table(html_nodes(web_page, "table")[[1]])
head(mountains_table,5)

```
Observation: The above codechunk pulls the first table from the link. Displaying first 5 records.

2. \textbf{\textcolor{red}{Solution:}}

###### Insert Response

```{r }

tables_all = html_table(html_nodes(web_page,"table"))
length(tables_all)  #display count of tables

```
Observation: All the tables are stored in a list form. There are a total of 9 tables in the given webpage.

3. \textbf{\textcolor{red}{Solution:}}

###### Insert Response

```{r }
#We can use FILL = TRUE to have headers in teh output
tables_all_updated = html_table(html_node(web_page,"table"),fill = TRUE)

lapply(tables_all_updated[1:3],names)

```
Now let's locate the mountain links for each mountain using pipe operator and other html functions like html_attr

SOURCE: https://rvest.tidyverse.org/reference/html_attr.html
```{r}

mountain_links = html_nodes(web_page, "table")[[1]] %>% html_nodes("a") %>% html_attr("href")


head(mountain_links)


```
Observation: We have successfully extracted a few mountain link names. Earlier, we found the headers of the table content.

4. \textbf{\textcolor{red}{Solution:}}

###### Insert Response


```{r }

all_tables_updated = html_nodes(web_page, "table")


# Pulling first 3 tables

mountain_tables = lapply(all_tables_updated[1:3],function(table) {
  df = html_table(table, fill = TRUE)
  df = df %>% rename(Name = 1, Height_m = 2)  #rename columns for standardization
  df = df %>% select(Name, Height_m)  #select  columns
  df = df %>% filter(Height_m>6800)
  return(df)
})


#Combine tables into one data frame
combined_df_raw = do.call(rbind, mountain_tables)

nrow(combined_df_raw) #number of rows

#let's extract mountain links for the first three tables
mountain_nodes= html_nodes(all_tables_updated[1:3],"tr a")
mountain_names= mountain_nodes %>% html_text()
mountain_links= mountain_nodes %>% html_attr("href")

#building a dataframe, consistency check and filtering
combined_df = combined_df_raw %>%
  mutate(
    Link = ifelse(Name %in% mountain_names,
                  paste0("https://en.wikipedia.org",mountain_links[match(Name, mountain_names)]),
                  NA)
  )
#display
summary(combined_df)
```
Observation: There a total of 148 mountains with valid links.


5. \textbf{\textcolor{red}{Solution:}}

###### Insert Response

```{r }

tail(combined_df,10)


```

Observation: Data looks reasonable

# 1.2 Scrape the individual mountain data (25pt)

For this step, we recommend to write a function that takes the link as an argument, and scrapes the mountains’ data. The coordinates are given in degrees-minutes-seconds east/west, north/south; so you also need a function to transform those to degrees (east/north is positive, west/south negative) to match the plotting below.

However, you do not have to follow our suggestions if you have a better idea.

1. (4pt) Write a function that converts the longitude/latitude string to degrees (positive and negative).
  Hint: here is code that converts coordinates like 28 35’46"N or 83 49’13"E into degrees (note: you have to ensure you are using the exact same degree-minute-second symbols as in wikipedia):

```{r}

ddmmssDD = "28 35’46\"N"  # Example coordinate, replace with your own
sampel = "35°52′57″N"
# Determine direction: west/south are negative
D <- if (grepl("[WS]", ddmmssDD)) -1 else 1

# Use the exact symbols as in Wikipedia for splitting
# Split by spaces for degrees and minutes, and handle the special symbol for minutes ’ and seconds "
dms <- strsplit(ddmmssDD, "[ ’\"]")[[1]]

# Extract degrees, minutes, and seconds as numeric values
dd <- as.numeric(dms[1]) 
mm <- as.numeric(dms[2]) 
ss <- as.numeric(dms[3]) 

# Convert to decimal degrees
decimal_degrees <- (dd + mm/60 + ss/3600) * D

```


2. (10pt) Write another function that takes link as an argument and loads the mountain’s html page and extracts latitude and longitude.
  Hint: you may run intro trouble with certain links (in particular non-existing wiki pages). In that case you may want to wrap the download code into try-block like this:
  
  
```{r}
page <- try(read_html(url), silent=TRUE)
                          # if it works, you get the page
                          # if not, you get "try-error"
                          # silent: do not show errors in markdown
if(inherits(page, "try-error"))
                          # got 404 or another error, return NULL
  return(NULL)
## Process the page here 
## ...
```


3. (7pt) loop over the table of mountains you did above, download the mountain data, and extract the coordinates. Store these into the same data frame.
Hint: I managed to download data for 152 mountains


4. (4pt) Print a sample of the dataframe and check that it looks good. How many mountains did you get?

##### Solution 1.2

1. \textbf{\textcolor{red}{Solution:}}

###### Insert Response

Source: https://stackoverflow.com/questions/3518504/regular-expression-for-matching-latitude-longitude-coordinates

```{r}

convert_to_decimal = function(ddmmssDD) {
  #direction: west or south are negative
  D = if (grepl("[WS]", ddmmssDD)) -1 else 1
  
  #Split the string into degrees, minutes and seconds 
  dms = strsplit(ddmmssDD, "[°′″ ]")[[1]]
  
  #extract degrees, minutes, and seconds 
  dd = as.numeric(dms[1]) 
  mm = as.numeric(dms[2]) 
  ss = as.numeric(dms[3]) 
  
  #convert to decimal degrees
  decimal_degrees = (dd + mm/60 + ss/3600) * D
  return(decimal_degrees)
}

#Testing
latitude = convert_to_decimal("35°52′57″N")
longitude= convert_to_decimal("76°30′48″E")

latitude
longitude



```
Observation: The code successfully converts latitudes and longitudes based on the given requirements


2. \textbf{\textcolor{red}{Solution:}}

###### Insert Response

try block captures the entries for which link is not available.

```{r}

#let's build a function that loads a mountain's Wikipedia page and pulls coordinates
fetch_coordinates= function(link) {
 
  page = try(read_html(link), silent = TRUE)
  
  # Check if success or failure
  if (inherits(page, "try-error")) {
    return(NULL)  #returning NULL if failure
  }
  
  #extract the coordinate elements
  lat_node = html_node(page, ".latitude")
  lon_node = html_node(page, ".longitude")
  
  lat_node
  lon_node
  #check if both latitude and longitude exist
  if (is.null(lat_node)||is.null(lon_node)) {
    return(NULL)  
  }
  
  lat_text = html_text(lat_node)
  lon_text = html_text(lon_node)
  
  #convert coordinates to decimal degrees using function created earlier
  latitude = convert_to_decimal(lat_text)
  longitude = convert_to_decimal(lon_text)
  
  return(list(latitude = latitude, longitude = longitude))
}

#test case
coordinates = fetch_coordinates("https://en.wikipedia.org/wiki/K2")

coordinates



```
Observation: We have successfully fetched coordinates from a direct link. We used helper functions created earlier to simplify our code. We can now parse the entire dataframe and join the coordinates on to the main table.


3. \textbf{\textcolor{red}{Solution:}}

###### Insert Response

```{r}

# adding latitude and longitude columns and assigning them NA values
combined_df=combined_df %>% 
  mutate(
    latitude = NA_real_,
    longitude = NA_real_
  )

#loop over each row 
for (i in 1:nrow(combined_df)) {
  #get link for the mountain
  link = combined_df$Link[i]
  
  if (!is.na(url)) {
    coords= fetch_coordinates(link)

    if (!is.null(coords)) {
      combined_df$latitude[i] <- coords$latitude
      combined_df$longitude[i] <- coords$longitude
    }
  }
}

# Display summary of the dataframe
summary(combined_df)



```
Observation: We have successfully added the columns of latitude and longitude. And now we have a total of 176 mountains to plot.

4. \textbf{\textcolor{red}{Solution:}}
```{r}

head(combined_df,5)
summary(combined_df)

```
Observation: 176 mountains and the dataframe has 5 columns now with latitude and longitude included.

# 1.3 Plot the mountains (12pt)

Finally, plot the mountains

1. (8pt) Plot all the mountains on a world map. Color those according to their height. Here is an example how to plot data on map:


```{r}
data <- data.frame(city=c("Vientiane", "Kinshasa"), 
                   longitude=c(102.5, 15.3),
                   latitude=c(18, -4.3))
world <- map_data("world") 
ggplot(world) +
  geom_polygon(aes(long, lat, group=group),
               col="white", fill="gray") +
  geom_point(data=data, aes(longitude, latitude)) + 
  coord_quickmap()
```

2. (4pt) Describe what did you get. Where are the tall mountains located? Do all the locations make sense (i.e. you do not have mountains in the middle of sea and such)?


##### Solution 1.3

1. \textbf{\textcolor{red}{Solution:}}

Source: https://www.youtube.com/watch?v=FoqiFR5ZCic
```{r}
#let's first remove any rows with NA coordinates to avoid any errors later
mountains_data = combined_df %>% 
  filter(!is.na(latitude) & !is.na(longitude))

world=map_data("world")

ggplot(world) +
  geom_polygon(aes(x = long, y = lat, group = group), color = "black", fill = "gray") +
  geom_point(data = mountains_data, aes(x = longitude, y = latitude, color = as.numeric(Height_m)), size = 2) +
  scale_color_gradient(low = "purple", high = "red", name = "Height (m)") +
  coord_quickmap() +
  labs(title = "Mountains that are greater than 6800m in height", x = "Longitude", y = "Latitude") 
  

```

###### Insert Response

2. \textbf{\textcolor{red}{Solution:}}

It can be observed that a vast majority of mountains are at the intersection of Indian  plate and the Eurasian plate. This could potentially signify the high level of techtonic activity that can occur near the area. Also, there are a very few such mountains in china mainland as well.

###### Insert Response

# Problem 2: Asking Data Science Questions: Crime and Educational Attainment (25pt)
lem Set 3, you joined data about crimes and educational attainment. Here you will use this new combined dataset to examine questions around crimes in Seattle and the educational attainment of people
living in the areas in which the crime occurred. 
In Prob

#### (a) (5pt) Develop a Data Science Question

Develop your own question to address in this analysis. Your question should be specific and measurable, and it should be able to be addressed through a basic analysis of the crime dataset you compiled in Problem Set 3. 

\textbf{\textcolor{red}{Solution:}}

###### Insert Response

```{r}
crime_beats_census = read_csv("crime_beats_census.csv")

```
Possible Data Science Question: 

### Q.How does the educational attainment of individuals involved in crimes differ along different crime subcategories in Seattle?

#### (b) (5pt) Describe and Summarize

Briefly summarize the dataset, describing what data exists and its basic properties. Comment on any issues that need to be resolved before you can proceed with your analysis. 

\textbf{\textcolor{red}{Solution:}}

###### Insert Response

```{r}
#let's see basic descriptive statistics of this dataset
summary(crime_beats_census)

#check for missing values
missing_values = sum(is.na(crime_beats_census))
cat("Total missing values:", missing_values, "\n")

#checking outliers in numerical columns
summary(crime_beats_census$Latitude)
summary(crime_beats_census$Longitude)


```
Observation: There are a lot of missing values (mostly from NA's in the educational attainment columns as we went ahead with one hot encoding method). No outliers found in latitude and longitude. But there are NA's.

#### (c) (10pt) Data Analysis

Use the dataset to provide empirical evidence that addressed your question from part (a). Discuss your results. Provide at least one visualization to support your narrative. 

\textbf{\textcolor{red}{Solution:}}

###### Insert Response

Approach: Group data by crime subcategories and calculate the average educational attainment for significant levels and visualize.

```{r}
#filter out NA's

colnames(crime_beats_census)
crime_df = crime_beats_census %>% 
  filter(!is.na("Crime Subcategory") & !is.na(high_school_diploma) & !is.na(bachelors_degree))

crime_df = crime_df %>%
  rename(Crime_Subcategory = 'Crime Subcategory')


#aggregate data, average educational attainment by crime subcategory
education_impact = crime_df %>%
  group_by(Crime_Subcategory) %>%
  summarise(
    Average_High_School =mean(high_school_diploma, na.rm = TRUE),
    Average_Bachelors =mean(bachelors_degree, na.rm = TRUE),
    Average_Masters = mean(masters_degree, na.rm = TRUE),
    .groups = 'drop'
  )


#plotting the graph
ggplot(education_impact, aes(x = reorder(Crime_Subcategory, -Average_Bachelors), y = Average_Bachelors)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(
    title = "Average Bachelors Degree Holders by Crime subcategory",
    x= "Crime Subcategory",
    y = "Avg Count of Bachelor's Degree holders"
  ) +
  theme_minimal() +
  theme(axis.text.x =element_text(angle = 45,hjust = 1))
```
Observation: 

#### (d) (5pt) Reflect and Question

Comment the questions (and answers) in this analysis.  Were you able to answer all of these questions?  Are all questions well defined?  Is the data good enough to answer all these?

\textbf{\textcolor{red}{Solution:}}

###### Insert Response

Individuals with higher educational attainments are less likely to engage in severe crime activities. Thsi is observed by the decreasing trend from left to right. Please note that the sharp increase in the end is for NA's.

Yes, The visualization supports the general hypothesis that higher educational attainments have a positive impact on crime reduction. While the question was clear and specific, clarification about socio-economic factors might be needed to explore other aspects.
```{r}
head(education_impact,4)

```

